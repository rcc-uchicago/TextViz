{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-06 17:02:46,505 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-05-06 17:02:47,008 : INFO : built Dictionary(23341 unique tokens: ['!', '(', ')', ',', '.']...) from 42 documents (total 1087136 corpus positions)\n",
      "2019-05-06 17:02:47,412 : INFO : collecting document frequencies\n",
      "2019-05-06 17:02:47,413 : INFO : PROGRESS: processing document #0\n",
      "2019-05-06 17:02:47,455 : INFO : calculating IDF weights for 42 documents and 23340 features (131075 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF example -- can be run independently\n",
    "\n",
    "# imports and set up logging\n",
    "import gensim \n",
    "import logging\n",
    "import glob, os\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from pprint import pprint\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# directory containing all source texts for training the model \n",
    "data_dir=\"../corpus\"\n",
    "os.chdir(data_dir)\n",
    "\n",
    "lemmatized_corpus = []   \n",
    "original_corpus = []     \n",
    "\n",
    "tfdocuments = []\n",
    "for filename in glob.glob(\"*.txt\"):\n",
    "    filedata = open(filename, 'r').read()\n",
    "    tfdocuments.append(filedata)\n",
    "\n",
    "print(\"Number of documents: \" + str(len(tfdocuments)))\n",
    "    \n",
    "for xdoc in tfdocuments:               \n",
    "    tokens = word_tokenize(xdoc)     \n",
    "    lemmas = [nltk.stem.WordNetLemmatizer().lemmatize(token.lower()) for token in tokens]\n",
    "    lemmatized_corpus.append(lemmas)    \n",
    "    original_corpus.append(tokens)    \n",
    "\n",
    "dictionary = Dictionary(lemmatized_corpus)   # Build the dictionary\n",
    "\n",
    "# Convert to vector corpus\n",
    "vectors = [dictionary.doc2bow(text) for text in lemmatized_corpus]\n",
    "\n",
    "# Build TF-IDF model\n",
    "tfidf = TfidfModel(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('percy', 0.3778585558451681),\n",
      " ('douglas', 0.2658940827268987),\n",
      " ('harry', 0.2259219125965405),\n",
      " ('mortimer', 0.22178654364825082),\n",
      " ('falstaff', 0.17250064505975066),\n",
      " ('glendower', 0.13964337933408386),\n",
      " ('francis', 0.13770568627722468),\n",
      " ('scot', 0.13142906290266715),\n",
      " ('john', 0.12589087534249327),\n",
      " ('wale', 0.12518698752474972),\n",
      " ('poins', 0.11063169225788345),\n",
      " ('westmoreland', 0.11063169225788345),\n",
      " ('jack', 0.100638582605262),\n",
      " ('worcester', 0.09857179717700036),\n",
      " ('ostler', 0.09140047130714636),\n",
      " ('hotspur', 0.08508610647260757),\n",
      " ('owen', 0.08508610647260757),\n",
      " ('sack', 0.08193583970552355),\n",
      " ('buckram', 0.07445034316353162),\n",
      " ('peto', 0.07445034316353162),\n",
      " ('bardolph', 0.07392884788275027),\n",
      " ('welsh', 0.07392884788275027),\n",
      " ('kate', 0.06885284313861234),\n",
      " ('zounds', 0.06797860810497708),\n",
      " ('holmedon', 0.06528605093367597),\n",
      " ('sheriff', 0.06453515381709868),\n",
      " ('ned', 0.06118074729447937),\n",
      " ('i', 0.06044311631758438),\n",
      " ('sblood', 0.05947827720976723),\n",
      " ('instinct', 0.05633414438613737),\n",
      " ('lancaster', 0.05633414438613737),\n",
      " ('cousin', 0.05437077347119838),\n",
      " ('butter', 0.05317881654537973),\n",
      " ('eastcheap', 0.05317881654537973),\n",
      " ('severn', 0.05317881654537973),\n",
      " ('…', 0.05317881654537973),\n",
      " ('gadshill', 0.05222884074694078),\n",
      " ('mordake', 0.05222884074694078),\n",
      " ('bolingbroke', 0.052043492558546324),\n",
      " ('walter', 0.04928589858850018),\n",
      " ('shrewsbury', 0.046096538440784776),\n",
      " ('rogue', 0.04600749067552572),\n",
      " ('trent', 0.042543053236303786),\n",
      " ('vernon', 0.042543053236303786),\n",
      " ('michael', 0.04107158215708349),\n",
      " ('northumberland', 0.04055008687630214),\n",
      " ('tavern', 0.04055008687630214),\n",
      " ('villainous', 0.039387725395546136),\n",
      " ('a-horseback', 0.03917163056020558)]\n"
     ]
    }
   ],
   "source": [
    "# Get TF-IDF weights\n",
    "weights = tfidf[vectors[0]]\n",
    "\n",
    "# Get terms from the dictionary and pair with weights\n",
    "weights = [(dictionary[pair[0]], pair[1]) for pair in weights]\n",
    "\n",
    "pprint(sorted(weights, key=lambda weights: weights[1], reverse=True)[1:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1H4_h.txt', 46],\n",
       " ['1H6_h.txt', 1],\n",
       " ['2H4_h.txt', 8],\n",
       " ['2H6_h.txt', 0],\n",
       " ['3H6_h.txt', 0],\n",
       " ['Ado_c.txt', 0],\n",
       " ['Ant_t.txt', 0],\n",
       " ['AWW_c.txt', 0],\n",
       " ['AYL_c.txt', 0],\n",
       " ['Cor_t.txt', 0],\n",
       " ['Cym_t.txt', 0],\n",
       " ['Err_c.txt', 0],\n",
       " ['H5_h.txt', 0],\n",
       " ['H8_h.txt', 0],\n",
       " ['Ham_t.txt', 0],\n",
       " ['JC_t.txt', 0],\n",
       " ['John_t.txt', 0],\n",
       " ['Lear_t.txt', 0],\n",
       " ['LLL_c.txt', 0],\n",
       " ['Lucrece_x.txt', 0],\n",
       " ['M4M_c.txt', 0],\n",
       " ['Mac_t.txt', 0],\n",
       " ['MerchV_c.txt', 0],\n",
       " ['MND_c.txt', 0],\n",
       " ['Oth_t.txt', 0],\n",
       " ['Pericles_x.txt', 0],\n",
       " ['PhxTur_x.txt', 0],\n",
       " ['R2_h.txt', 4],\n",
       " ['R3_h.txt', 0],\n",
       " ['Rom_t.txt', 0],\n",
       " ['Shr_c.txt', 0],\n",
       " ['Sonnets_x.txt', 0],\n",
       " ['TGV_c.txt', 0],\n",
       " ['Tim_t.txt', 0],\n",
       " ['Tit_t.txt', 0],\n",
       " ['Tmp_c.txt', 0],\n",
       " ['TN_c.txt', 0],\n",
       " ['TNK_x.txt', 0],\n",
       " ['Tro_c.txt', 0],\n",
       " ['VenusAdonis_x.txt', 0],\n",
       " ['Wiv_c.txt', 0],\n",
       " ['WT_c.txt', 0]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the answer Part 1: \n",
    "# See in which document (=file) the word occurs, and return the raw number of tokens\n",
    "\n",
    "def word_freq(search_term):\n",
    "    lemmacounts = []\n",
    "    for filename in glob.glob(\"*.txt\"):\n",
    "        filedata = open(filename, 'r').read()\n",
    "        tokens = word_tokenize(filedata)     \n",
    "        lemmas = [nltk.stem.WordNetLemmatizer().lemmatize(token.lower()) for token in tokens]\n",
    "        lemmacounts.append([filename, lemmas.count(search_term)])\n",
    "    return lemmacounts\n",
    "        \n",
    "word_freq(\"percy\")  #douglas #king"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In file 1H4_h.txt, the term 'percy' exists 46 times compared to 29288 total lemmatized tokens for a raw frequency of 0.0015706091231903851\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the answer Part 2: \n",
    "#   Open the document (=file) and print the number of times the word appears,\n",
    "#   the total number of tokens, and the term frequency relative to the total.\n",
    "\n",
    "# directory containing all source texts for training the model \n",
    "data_dir=\"../corpus\"\n",
    "os.chdir(data_dir)\n",
    "\n",
    "def freq_check(filename, search_term):\n",
    "    filedata = open(filename, 'r').read()\n",
    "    tokens = word_tokenize(filedata)     \n",
    "    lemmas = [nltk.stem.WordNetLemmatizer().lemmatize(token.lower()) for token in tokens]\n",
    "    resultstr = \"In file \" + filename + \", the term '\" + search_term + \"' exists \" + str(lemmas.count(search_term)) + \" times\" + \\\n",
    "    \" compared to \" + str(len(lemmas)) + \" total lemmatized tokens for a raw frequency of \" + str((lemmas.count(search_term) / len(lemmas)))\n",
    "    return resultstr\n",
    "\n",
    "freq_check('1H4_h.txt','percy')  #percy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens:\n",
      "[['’', 406],\n",
      " ['thou', 253],\n",
      " ['shall', 134],\n",
      " ['lord', 123],\n",
      " ['—', 118],\n",
      " ['thy', 111],\n",
      " ['thee', 102],\n",
      " ['good', 82],\n",
      " ['come', 80],\n",
      " ['king', 75],\n",
      " ['well', 74],\n",
      " ['us', 66],\n",
      " ['upon', 64],\n",
      " ['man', 61],\n",
      " ['like', 60],\n",
      " ['let', 59],\n",
      " ['art', 59],\n",
      " ['would', 58],\n",
      " ['sir', 51],\n",
      " ['hath', 51],\n",
      " ['tell', 49],\n",
      " ['god', 48],\n",
      " ['percy', 46],\n",
      " ['time', 45],\n",
      " ['yet', 44],\n",
      " ['make', 42],\n",
      " ['never', 42],\n",
      " ['know', 41],\n",
      " ['hear', 40],\n",
      " ['say', 40],\n",
      " ['give', 39],\n",
      " ['harry', 39],\n",
      " ['go', 37],\n",
      " ['hal', 37],\n",
      " ['men', 37],\n",
      " ['may', 37],\n",
      " ['hast', 36],\n",
      " ['john', 35],\n",
      " ['see', 34],\n",
      " ['jack', 34],\n",
      " ['think', 33],\n",
      " ['faith', 33],\n",
      " ['one', 33],\n",
      " ['father', 33],\n",
      " ['horse', 32],\n",
      " ['prince', 32],\n",
      " ['true', 31],\n",
      " ['must', 31],\n",
      " ['life', 31],\n",
      " ['day', 31]]\n",
      "\n",
      "Lemmas:\n",
      "[['’', 406],\n",
      " ['thou', 253],\n",
      " ['shall', 134],\n",
      " ['lord', 129],\n",
      " ['—', 118],\n",
      " ['thy', 111],\n",
      " ['thee', 102],\n",
      " ['come', 92],\n",
      " ['good', 82],\n",
      " ['king', 76],\n",
      " ['well', 74],\n",
      " ['u', 66],\n",
      " ['wa', 65],\n",
      " ['upon', 64],\n",
      " ['man', 61],\n",
      " ['like', 60],\n",
      " ['let', 59],\n",
      " ['art', 59],\n",
      " ['would', 58],\n",
      " ['sir', 54],\n",
      " ['time', 53],\n",
      " ['hath', 51],\n",
      " ['god', 49],\n",
      " ['tell', 49],\n",
      " ['say', 48],\n",
      " ['know', 46],\n",
      " ['percy', 46],\n",
      " ['make', 45],\n",
      " ['yet', 44],\n",
      " ['never', 42],\n",
      " ['go', 40],\n",
      " ['hear', 40],\n",
      " ['day', 40],\n",
      " ['give', 39],\n",
      " ['harry', 39],\n",
      " ['horse', 38],\n",
      " ['see', 37],\n",
      " ['hal', 37],\n",
      " ['men', 37],\n",
      " ['life', 37],\n",
      " ['may', 37],\n",
      " ['hast', 36],\n",
      " ['john', 35],\n",
      " ['prince', 34],\n",
      " ['jack', 34],\n",
      " ['think', 33],\n",
      " ['faith', 33],\n",
      " ['one', 33],\n",
      " ['father', 33],\n",
      " ['love', 32]]\n"
     ]
    }
   ],
   "source": [
    "# As a final check, provide the most frequent lemmas for that document (=file)\n",
    "from pprint import pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# directory containing all source texts for training the model \n",
    "data_dir=\"../corpus\"\n",
    "filename=\"1H4_h.txt\"\n",
    "os.chdir(data_dir)\n",
    "\n",
    "filedata = open(filename, 'r').read()\n",
    "xtokens = word_tokenize(filedata)     \n",
    "xlemmas = [nltk.stem.WordNetLemmatizer().lemmatize(token.lower()) for token in xtokens]\n",
    "\n",
    "# Stopwords and punctuation removal\n",
    "stop = stopwords.words('english') + list(string.punctuation)\n",
    "\n",
    "tokens_nsp = [i.lower() for i in xtokens if i.lower() not in stop]\n",
    "lemmas_nsp = [i.lower() for i in xlemmas if i.lower() not in stop]\n",
    "\n",
    "##\n",
    "\n",
    "print (\"\\nTokens:\")\n",
    "\n",
    "wfreq = []\n",
    "for w in set(tokens_nsp):\n",
    "    wfreq.append([w, tokens_nsp.count(w)])\n",
    "wfreq_sorted = sorted(wfreq, key=lambda wfreq: wfreq[1], reverse=True)\n",
    "pprint(wfreq_sorted[0:50])\n",
    "\n",
    "print (\"\\nLemmas:\")\n",
    "\n",
    "lfreq = []\n",
    "for w in set(lemmas_nsp):\n",
    "    lfreq.append([w, lemmas_nsp.count(w)])\n",
    "lfreq_sorted = sorted(lfreq, key=lambda lfreq: lfreq[1], reverse=True)\n",
    "pprint(lfreq_sorted[0:50])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
