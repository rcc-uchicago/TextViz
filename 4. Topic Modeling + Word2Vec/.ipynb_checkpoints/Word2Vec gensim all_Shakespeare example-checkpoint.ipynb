{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and set up logging\n",
    "import gensim \n",
    "import logging\n",
    "import glob, os\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory containing all source texts for training the model \n",
    "data_dir=\"../corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1H4_h.txt = 130423 chars\n",
      "1H6_h.txt = 116317 chars\n",
      "2H4_h.txt = 141574 chars\n",
      "2H6_h.txt = 135863 chars\n",
      "3H6_h.txt = 129551 chars\n",
      "Ado_c.txt = 111116 chars\n",
      "Ant_t.txt = 133668 chars\n",
      "AWW_c.txt = 121896 chars\n",
      "AYL_c.txt = 114429 chars\n",
      "Cor_t.txt = 146443 chars\n",
      "Cym_t.txt = 147159 chars\n",
      "Err_c.txt = 76924 chars\n",
      "H5_h.txt = 141819 chars\n",
      "H8_h.txt = 128223 chars\n",
      "Ham_t.txt = 163429 chars\n",
      "JC_t.txt = 104561 chars\n",
      "John_t.txt = 112414 chars\n",
      "Lear_t.txt = 140510 chars\n",
      "LLL_c.txt = 115391 chars\n",
      "Lucrece_x.txt = 86177 chars\n",
      "M4M_c.txt = 116348 chars\n",
      "Mac_t.txt = 91625 chars\n",
      "MerchV_c.txt = 112334 chars\n",
      "MND_c.txt = 88608 chars\n",
      "Oth_t.txt = 141395 chars\n",
      "Pericles_x.txt = 97471 chars\n",
      "PhxTur_x.txt = 2072 chars\n",
      "R2_h.txt = 120934 chars\n",
      "R3_h.txt = 156881 chars\n",
      "Rom_t.txt = 130885 chars\n",
      "Shr_c.txt = 111364 chars\n",
      "Sonnets_x.txt = 97204 chars\n",
      "TGV_c.txt = 91686 chars\n",
      "Tim_t.txt = 98749 chars\n",
      "Tit_t.txt = 109892 chars\n",
      "Tmp_c.txt = 88800 chars\n",
      "TN_c.txt = 104476 chars\n",
      "TNK_x.txt = 127691 chars\n",
      "Tro_c.txt = 142635 chars\n",
      "VenusAdonis_x.txt = 55527 chars\n",
      "Wiv_c.txt = 115202 chars\n",
      "WT_c.txt = 134528 chars\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_dir)\n",
    "documents = list()\n",
    "for filename in glob.glob(\"*.txt\"):\n",
    "    filedata = open(filename, 'r').read()\n",
    "    print(filename + \" = \" + str(len(filedata)) + \" chars\")\n",
    "    documents = documents + filedata.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " So shaken as we are , so wan with care , Find we a time for frighted peace to pant And breathe short-winded accents of new broils To be commenced in strands afar remote \n"
     ]
    }
   ],
   "source": [
    "# Check to see that the first sentence is correct\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-06 16:48:30,939 : INFO : collecting all words and their counts\n",
      "2019-05-06 16:48:30,940 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2019-05-06 16:48:31,324 : INFO : PROGRESS: at sentence #10000, processed 205010 words and 101389 word types\n",
      "2019-05-06 16:48:31,714 : INFO : PROGRESS: at sentence #20000, processed 403566 words and 170975 word types\n",
      "2019-05-06 16:48:32,110 : INFO : PROGRESS: at sentence #30000, processed 599588 words and 233068 word types\n",
      "2019-05-06 16:48:32,492 : INFO : PROGRESS: at sentence #40000, processed 798141 words and 286447 word types\n",
      "2019-05-06 16:48:32,884 : INFO : PROGRESS: at sentence #50000, processed 998259 words and 337166 word types\n",
      "2019-05-06 16:48:33,121 : INFO : collected 365018 word types from a corpus of 1107953 words (unigram + bigrams) and 55651 sentences\n",
      "2019-05-06 16:48:33,122 : INFO : using 365018 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2019-05-06 16:48:33,122 : INFO : source_vocab length 365018\n",
      "2019-05-06 16:48:36,908 : INFO : Phraser built with 1137 phrasegrams\n",
      "2019-05-06 16:48:36,921 : INFO : collecting all words and their counts\n",
      "2019-05-06 16:48:36,923 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2019-05-06 16:48:37,873 : INFO : PROGRESS: at sentence #10000, processed 177937 words and 100651 word types\n",
      "2019-05-06 16:48:38,777 : INFO : PROGRESS: at sentence #20000, processed 350165 words and 171156 word types\n",
      "2019-05-06 16:48:39,698 : INFO : PROGRESS: at sentence #30000, processed 520407 words and 233881 word types\n",
      "2019-05-06 16:48:40,592 : INFO : PROGRESS: at sentence #40000, processed 692661 words and 288512 word types\n",
      "2019-05-06 16:48:41,536 : INFO : PROGRESS: at sentence #50000, processed 866443 words and 340596 word types\n",
      "2019-05-06 16:48:42,073 : INFO : collected 369042 word types from a corpus of 961298 words (unigram + bigrams) and 55651 sentences\n",
      "2019-05-06 16:48:42,074 : INFO : using 369042 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2019-05-06 16:48:42,075 : INFO : source_vocab length 369042\n",
      "2019-05-06 16:48:46,292 : INFO : Phraser built with 2603 phrasegrams\n",
      "2019-05-06 16:48:55,797 : INFO : collecting all words and their counts\n",
      "2019-05-06 16:48:55,797 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-06 16:48:55,840 : INFO : PROGRESS: at sentence #10000, processed 175783 words, keeping 14778 word types\n",
      "2019-05-06 16:48:55,882 : INFO : PROGRESS: at sentence #20000, processed 346055 words, keeping 21104 word types\n",
      "2019-05-06 16:48:55,933 : INFO : PROGRESS: at sentence #30000, processed 514489 words, keeping 26209 word types\n",
      "2019-05-06 16:48:55,983 : INFO : PROGRESS: at sentence #40000, processed 684888 words, keeping 29786 word types\n",
      "2019-05-06 16:48:56,030 : INFO : PROGRESS: at sentence #50000, processed 857013 words, keeping 32922 word types\n",
      "2019-05-06 16:48:56,059 : INFO : collected 34965 word types from a corpus of 950847 raw words and 55651 sentences\n",
      "2019-05-06 16:48:56,060 : INFO : Loading a fresh vocabulary\n",
      "2019-05-06 16:48:56,133 : INFO : effective_min_count=1 retains 34965 unique words (100% of original 34965, drops 0)\n",
      "2019-05-06 16:48:56,134 : INFO : effective_min_count=1 leaves 950847 word corpus (100% of original 950847, drops 0)\n",
      "2019-05-06 16:48:56,231 : INFO : deleting the raw counts dictionary of 34965 items\n",
      "2019-05-06 16:48:56,233 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2019-05-06 16:48:56,234 : INFO : downsampling leaves estimated 711826 word corpus (74.9% of prior 950847)\n",
      "2019-05-06 16:48:56,356 : INFO : estimated required memory for 34965 words and 200 dimensions: 73426500 bytes\n",
      "2019-05-06 16:48:56,357 : INFO : resetting layer weights\n",
      "2019-05-06 16:48:56,900 : INFO : training model with 20 workers on 34965 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-05-06 16:48:57,464 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2019-05-06 16:48:57,477 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2019-05-06 16:48:57,493 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2019-05-06 16:48:57,499 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2019-05-06 16:48:57,512 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2019-05-06 16:48:57,518 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2019-05-06 16:48:57,522 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2019-05-06 16:48:57,525 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2019-05-06 16:48:57,526 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-05-06 16:48:57,527 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-05-06 16:48:57,528 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-06 16:48:57,538 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-06 16:48:57,539 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-06 16:48:57,542 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-06 16:48:57,543 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-06 16:48:57,544 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-06 16:48:57,545 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-06 16:48:57,549 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-06 16:48:57,550 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-06 16:48:57,554 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-06 16:48:57,554 : INFO : EPOCH - 1 : training on 950847 raw words (712089 effective words) took 0.6s, 1114992 effective words/s\n",
      "2019-05-06 16:48:58,119 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2019-05-06 16:48:58,124 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2019-05-06 16:48:58,125 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2019-05-06 16:48:58,137 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2019-05-06 16:48:58,143 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2019-05-06 16:48:58,163 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2019-05-06 16:48:58,171 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2019-05-06 16:48:58,179 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2019-05-06 16:48:58,183 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-05-06 16:48:58,189 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-05-06 16:48:58,192 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-06 16:48:58,194 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-06 16:48:58,196 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-06 16:48:58,198 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-06 16:48:58,198 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-06 16:48:58,205 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-06 16:48:58,208 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-06 16:48:58,211 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-06 16:48:58,223 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-06 16:48:58,224 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-06 16:48:58,225 : INFO : EPOCH - 2 : training on 950847 raw words (711943 effective words) took 0.7s, 1084830 effective words/s\n",
      "2019-05-06 16:48:58,868 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2019-05-06 16:48:58,876 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2019-05-06 16:48:58,879 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2019-05-06 16:48:58,892 : INFO : worker thread finished; awaiting finish of 16 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-06 16:48:58,894 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2019-05-06 16:48:58,896 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2019-05-06 16:48:58,899 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2019-05-06 16:48:58,912 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2019-05-06 16:48:58,914 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-05-06 16:48:58,921 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-05-06 16:48:58,930 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-06 16:48:58,933 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-06 16:48:58,939 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-06 16:48:58,943 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-06 16:48:58,945 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-06 16:48:58,947 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-06 16:48:58,952 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-06 16:48:58,956 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-06 16:48:58,958 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-06 16:48:58,962 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-06 16:48:58,963 : INFO : EPOCH - 3 : training on 950847 raw words (711670 effective words) took 0.7s, 985454 effective words/s\n",
      "2019-05-06 16:48:59,585 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2019-05-06 16:48:59,592 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2019-05-06 16:48:59,601 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2019-05-06 16:48:59,617 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2019-05-06 16:48:59,621 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2019-05-06 16:48:59,630 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2019-05-06 16:48:59,639 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2019-05-06 16:48:59,647 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2019-05-06 16:48:59,655 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-05-06 16:48:59,656 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-05-06 16:48:59,657 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-06 16:48:59,659 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-06 16:48:59,662 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-06 16:48:59,665 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-06 16:48:59,670 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-06 16:48:59,677 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-06 16:48:59,681 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-06 16:48:59,684 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-06 16:48:59,684 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-06 16:48:59,686 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-06 16:48:59,687 : INFO : EPOCH - 4 : training on 950847 raw words (712065 effective words) took 0.7s, 1004685 effective words/s\n",
      "2019-05-06 16:49:00,282 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2019-05-06 16:49:00,285 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2019-05-06 16:49:00,287 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2019-05-06 16:49:00,307 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2019-05-06 16:49:00,316 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2019-05-06 16:49:00,322 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2019-05-06 16:49:00,324 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2019-05-06 16:49:00,342 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2019-05-06 16:49:00,344 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2019-05-06 16:49:00,348 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2019-05-06 16:49:00,352 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-06 16:49:00,355 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-06 16:49:00,361 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-06 16:49:00,365 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-06 16:49:00,368 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-06 16:49:00,371 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-06 16:49:00,372 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-06 16:49:00,374 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-06 16:49:00,377 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-06 16:49:00,384 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-06 16:49:00,385 : INFO : EPOCH - 5 : training on 950847 raw words (712098 effective words) took 0.7s, 1040061 effective words/s\n",
      "2019-05-06 16:49:00,386 : INFO : training on a 4754235 raw words (3559865 effective words) took 3.5s, 1021720 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So', 'shaken', 'as', 'we_are', ',', 'so', 'wan', 'with', 'care', 'Find']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "#documents = [\"the mayor of new york was there\", \"human computer interaction and machine learning has now become a trending research area\",\"human computer interaction is interesting\",\"human computer interaction is a pretty interesting subject\", \"human computer interaction is a great and new subject\", \"machine learning can be useful sometimes\",\"new york mayor was present\", \"I love machine learning because it is a new subject area\", \"human computer interaction helps people to get user friendly applications\"]\n",
    "\n",
    "sentence_stream = [doc.split(\" \") for doc in documents]\n",
    "\n",
    "trigram_sentences_project = []\n",
    "\n",
    "#bigram = Phrases(sentence_stream, min_count=1, delimiter=b' ')\n",
    "#trigram = Phrases(bigram[sentence_stream], min_count=1, delimiter=b' ')\n",
    "#bigram = Phrases(sentence_stream, min_count=1, delimiter=b' ', threshold=2)\n",
    "#trigram = Phrases(bigram[sentence_stream], min_count=1, delimiter=b' ') #, threshold=3\n",
    "\n",
    "bigram = Phraser(Phrases(sentence_stream))\n",
    "trigram = Phraser(Phrases(bigram[sentence_stream]))\n",
    "\n",
    "for sent in sentence_stream:\n",
    "    bigrams_ = bigram[sent]\n",
    "    trigrams_ = trigram[bigram[sent]]\n",
    "    trigram_sentences_project.append(trigrams_)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 200    # Word vector dimensionality                      \n",
    "min_word_count = 1    # Minimum word count                        \n",
    "num_workers = 20      # Number of threads to run in parallel\n",
    "context = 5           # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "model = word2vec.Word2Vec(trigram_sentences_project, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "vocab = list(model.wv.vocab.keys())\n",
    "print(vocab[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34965\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of items in our model's vocabulary\n",
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-06 16:49:00,404 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('maid', 0.9613003134727478),\n",
       " ('gentleman', 0.9461257457733154),\n",
       " ('fault', 0.9242867827415466),\n",
       " ('rendezvous', 0.9154200553894043),\n",
       " ('prince', 0.9144469499588013),\n",
       " ('child', 0.9115806818008423),\n",
       " ('fellow', 0.9101417660713196),\n",
       " ('friend', 0.9053359627723694),\n",
       " ('true', 0.9029505252838135),\n",
       " ('wife', 0.8973380327224731)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"king\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fellow', 0.9325264692306519),\n",
       " ('lord', 0.9294525384902954),\n",
       " ('boy', 0.9263765811920166),\n",
       " ('cousin', 0.9183440804481506),\n",
       " ('son', 0.9147166609764099),\n",
       " ('master', 0.9087010622024536),\n",
       " ('fool', 0.9050697088241577),\n",
       " ('good_lord', 0.901745080947876),\n",
       " ('friend', 0.8994091153144836),\n",
       " ('daughter', 0.8966237902641296)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"lady\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('presumption', 0.930343747138977),\n",
       " ('beauty', 0.9223254919052124),\n",
       " ('foot', 0.9221459627151489),\n",
       " ('honor', 0.9213413596153259),\n",
       " ('life', 0.9172424077987671),\n",
       " ('revenge', 0.9150896072387695),\n",
       " ('strength', 0.9120564460754395),\n",
       " ('light', 0.9118926525115967),\n",
       " ('part', 0.9117753505706787),\n",
       " ('state', 0.9116936326026917)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"death\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1017689862109841\n",
      "0.056147770431868985\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.relative_cosine_similarity(\"lady\",\"lord\",topn=10))\n",
    "print(model.wv.relative_cosine_similarity(\"lady\",\"meal\",topn=10))\n",
    "# From the gensim documentation: \"For WordNet synonyms, if rcs(topn=10) is greater than 0.10 \n",
    "# then wa and wb are more similar than any arbitrary word pairs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inherits', 0.9226574897766113),\n",
       " ('youth’s', 0.9190940856933594),\n",
       " ('spied', 0.9032777547836304),\n",
       " ('joyless', 0.8993374705314636),\n",
       " ('wears', 0.8983495831489563),\n",
       " ('controlled', 0.8980189561843872),\n",
       " ('hellish', 0.8969630002975464),\n",
       " ('steals', 0.8960257768630981),\n",
       " ('throws', 0.8956001400947571),\n",
       " ('heaved', 0.8953067660331726)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analogies -- this example asks:\n",
    "# \"she\" is to \"sings\" as \"he\" is to ...   (analogies are often written like this: \"she:sings::he:?\")\n",
    "model.wv.most_similar(positive=['she','sings'],negative=['he'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('know', 0.004852022),\n",
       " ('think', 0.0039924937),\n",
       " ('believe', 0.003886514),\n",
       " ('do', 0.0034096413),\n",
       " ('would', 0.0033224535),\n",
       " ('thee', 0.002562578),\n",
       " ('love', 0.0025118762),\n",
       " ('If', 0.0024457485),\n",
       " ('can', 0.0022285825),\n",
       " ('myself', 0.0019670338),\n",
       " ('will', 0.0017480714),\n",
       " ('cannot', 0.0016750216),\n",
       " ('doubt', 0.0016237388),\n",
       " ('swear', 0.0014936058),\n",
       " ('dare_not', 0.0013485147),\n",
       " ('remember', 0.0013429864),\n",
       " ('so_much', 0.0013386527),\n",
       " ('understand', 0.0012973837),\n",
       " ('wish', 0.0012057007),\n",
       " ('what', 0.0011584938),\n",
       " ('must', 0.0011211802),\n",
       " ('well', 0.0010877775),\n",
       " ('hate', 0.0010768861),\n",
       " ('knew', 0.0010739639),\n",
       " ('might', 0.0010310024),\n",
       " ('I_am_sorry', 0.0009820322),\n",
       " ('shall', 0.0009516017),\n",
       " ('may', 0.00094604376),\n",
       " ('As', 0.00093095563),\n",
       " ('if', 0.00092470564)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_output_word([\"I\",\"do\",\"love\"], topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
